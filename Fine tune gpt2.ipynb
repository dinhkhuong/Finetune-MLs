{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa3f2be3-771a-44d9-853e-dc9e0c78b46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from datasets import load_dataset, Split\n",
    "from transformers import ( Trainer, AutoModelForCausalLM, AutoTokenizer,TrainingArguments )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2068fed-d497-4699-a2af-6a54a9f36379",
   "metadata": {},
   "outputs": [],
   "source": [
    "localfolder = 'texts'\n",
    "#download_text=('https://ota.bodleian.ox.ac.uk/repository/xmlui/bitstream/handle/20.500.12024/1476/alice28-1476.txt', localfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e66eb61-3279-49fe-887a-dc4cce1408b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(localfolder, 'alice28-1476.txt'), 'r') as f:\n",
    "    alice = ''.join(f.readlines()[0:3704])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04f60a12-0b0d-49d3-ba6c-cdbb7e68ce13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tALICE'S ADVENTURES IN WONDERLAND\n",
      "\n",
      "                          Lewis Carroll\n",
      "\n",
      "               THE MILLENNIUM FULCRUM EDITION 2.8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                            CHAPTER I\n",
      "\n",
      "                      Down the Rabbit-Hole\n",
      "\n",
      "\n",
      "  Alice was beginning to get very tired of sitting by her sister\n",
      "on the bank, and of having nothing to do:  once or twice she had\n",
      "peeped into the book her sister was reading, but it had no\n",
      "pictures or conversations in it, `and what is the use of a book,'\n",
      "thought Alice `without picture\n"
     ]
    }
   ],
   "source": [
    "print(alice[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c9f6342-d485-42f7-bf31-b7997f91ffe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Khuong\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Khuong\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96464e7b-8149-4636-ae20-4f71f0681728",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_alice = sent_tokenize(alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "103b5af2-68a2-410e-9c14-be8ba7fbc03a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1612"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32db404b-a74c-4d78-afbb-5d1df1c2ccb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There was nothing so VERY remarkable in that; nor did Alice\\nthink it so VERY much out of the way to hear the Rabbit say to\\nitself, `Oh dear!'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_alice[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2528966-4b89-4d5d-bcff-5771908224c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_tokenize(source, quote_char='\\\\', sep_char=',',\n",
    "                      include_header=True, include_source=True,\n",
    "                      extensions=('txt'), **kwargs):\n",
    "    nltk.download('punkt')\n",
    "    # If source is a folder, goes through all files inside it\n",
    "    # that match the desired extensions ('txt' by default)\n",
    "    if os.path.isdir(source):\n",
    "        filenames = [f for f in os.listdir(source)\n",
    "                     if os.path.isfile(os.path.join(source, f)) and\n",
    "                        os.path.splitext(f)[1][1:] in extensions]\n",
    "    elif isinstance(source, str):\n",
    "        filenames = [source]\n",
    "\n",
    "    # If there is a configuration file, builds a dictionary with\n",
    "    # the corresponding start and end lines of each text file\n",
    "    config_file = os.path.join(source, 'lines.cfg')\n",
    "    config = {}\n",
    "    if os.path.exists(config_file):\n",
    "        with open(config_file, 'r') as f:\n",
    "            rows = f.readlines()\n",
    "\n",
    "        for r in rows[1:]:\n",
    "            fname, start, end = r.strip().split(',')\n",
    "            config.update({fname: (int(start), int(end))})\n",
    "\n",
    "    new_fnames = []\n",
    "    # For each file of text\n",
    "    for fname in filenames:\n",
    "        # If there's a start and end line for that file, use it\n",
    "        try:\n",
    "            start, end = config[fname]\n",
    "        except KeyError:\n",
    "            start = None\n",
    "            end = None\n",
    "\n",
    "        # Opens the file, slices the configures lines (if any)\n",
    "        # cleans line breaks and uses the sentence tokenizer\n",
    "        with open(os.path.join(source, fname), 'r') as f:\n",
    "            contents = (''.join(f.readlines()[slice(start, end, None)])\n",
    "                        .replace('\\n', ' ').replace('\\r', ''))\n",
    "        corpus = sent_tokenize(contents, **kwargs)\n",
    "\n",
    "        # Builds a CSV file containing tokenized sentences\n",
    "        base = os.path.splitext(fname)[0]\n",
    "        new_fname = f'{base}.sent.csv'\n",
    "        new_fname = os.path.join(source, new_fname)\n",
    "        with open(new_fname, 'w') as f:\n",
    "            # Header of the file\n",
    "            if include_header:\n",
    "                if include_source:\n",
    "                    f.write('sentence,source\\n')\n",
    "                else:\n",
    "                    f.write('sentence\\n')\n",
    "            # Writes one line for each sentence\n",
    "            for sentence in corpus:\n",
    "                if include_source:\n",
    "                    f.write(f'{quote_char}{sentence}{quote_char}{sep_char}{fname}\\n')\n",
    "                else:\n",
    "                    f.write(f'{quote_char}{sentence}{quote_char}\\n')\n",
    "        new_fnames.append(new_fname)\n",
    "\n",
    "    # Returns list of the newly generated CSV files\n",
    "    return sorted(new_fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9bbd26c-9e11-4221-9583-efe8d194812c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Khuong\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "new_fnames = sentence_tokenize(localfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87f6059c-0abe-4178-812b-07fc34057bf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['texts\\\\alice28-1476.sent.csv']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e2d4696-98d4-4213-a3e5-9fa4fc6c60f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf89f54ccef4fff897555a91b5fd258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(path='csv', data_files=['texts/alice28-1476.sent.csv'], quotechar='\\\\', split=Split.TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef0c555c-18a6-44e2-b06b-4e83efb0bd51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her.',\n",
       " 'source': 'alice28-1476.txt'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7a63459b-67c1-40cf-8f6b-70e96fad4aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python38\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e0b6e57c-0bfb-4322-89b3-56587f633501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python38\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b0418208a164b78a4c161c8495d9bc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2865b1c4-5aca-4536-b4c8-29e0afaf7b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"_name_or_path\": \"gpt2\",\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"pad_token_id\": 50256,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.27.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f36666bd-c89e-4716-9d9a-774e5dc6faf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_dataset = dataset.shuffle(seed=42)\n",
    "split_dataset = shuffled_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset, test_dataset = split_dataset['train'], split_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "293aa0ec-1329-40db-897a-7f0bdc7c36f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "def tokenize(row):\n",
    "    return auto_tokenizer(row['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "30dfb608-c469-45b7-8929-49c270e33ae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e66704c981c945ac9490fecc9a75c6cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1289 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1cfc2f9d2054614ab1873c34ee8e135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/323 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(tokenize, remove_columns=['source', 'sentence'], batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize, remove_columns=['source', 'sentence'], batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5a5e3c22-fdba-4ae4-a454-48eb12ac4288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 28, 20, 9, 34, 29]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(len, tokenized_train_dataset[0:6]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "163aae19-4ea0-4d21-9e50-0c745e0a8bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_clm.py\n",
    "def group_texts(examples, block_size=128):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "71d6be05-dc6e-4622-8147-5c9f5f62bc47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "549ff7deacfc4dfa80168c93bbcd408d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1289 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d2b3cda6634751876b07ed143c3ce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/323 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_train_dataset = tokenized_train_dataset.map(group_texts, batched=True)\n",
    "lm_test_dataset = tokenized_test_dataset.map(group_texts, batched=True)\n",
    "lm_train_dataset.set_format(type='torch')\n",
    "lm_test_dataset.set_format(type='torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "80169295-7be0-4212-91a7-00075c19b3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   63,  2437,   466,   345,   760,   314,  1101,  8805,  8348,   464,\n",
      "         2677,  3114,  7296,  6819,   379,   262,  2635, 25498,    11,   508,\n",
      "          531,   287,   257,  1877,  3809,    11,  4600,  7120, 25788,  1276,\n",
      "         3272,    12,  1069,  9862, 12680,  4973,  2637,  1537,   611,   314,\n",
      "         1101,   407,   262,   976,    11,   262,  1306,  1808,   318,    11,\n",
      "         5338,   287,   262,   995,   716,   314,    30,   464,   360,   579,\n",
      "         1076,  6364,  4721,   465,  2951,    13,    63,  1026,   373,   881,\n",
      "        21289,   272,   353,   379,  1363,  4032,  1807,  3595, 14862,    11,\n",
      "         4600, 12518,   530,  2492,   470,  1464,  3957,  4025,   290,  4833,\n",
      "           11,   290,   852,  6149,   546,   416, 10693,   290, 33043,    13,\n",
      "         1870, 14862,   373,   523,   881, 24776,   326,   673,  4966,   572,\n",
      "          379,  1752,   287,   262,  4571,   340,  6235,   284,    11,  1231,\n",
      "         2111,   284,  4727,   262,  7457,   340,   550,   925])\n"
     ]
    }
   ],
   "source": [
    "print(lm_train_dataset[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ef5491bd-d7bb-464f-a181-e620467cc18d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(239, 56)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm_train_dataset), len(lm_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c33b31c0-cdbb-450c-a983-5b25f7dee39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "print(model.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6f1a0d9d-f901-4004-a543-37b8eb36a127",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='output',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=50,\n",
    "    logging_steps=50,\n",
    "    gradient_accumulation_steps=4,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  args=training_args,\n",
    "                  train_dataset=lm_train_dataset,\n",
    "                  eval_dataset=lm_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281a90b0-5a03-4eef-9668-12f521a5199c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python38\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='59' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/59 01:34 < 02:00, 0.27 it/s, Epoch 0.44/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d5081a-7d8c-4ae4-adb7-f0cc1fc5744c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cd227a-d600-4d18-8515-d679f49327b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_index = model.device.index if model.device.type != 'cpu' else -1\n",
    "gpt2_gen = pipeline('text-generation', model=model, tokenizer=auto_tokenizer, device=device_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156ae56c-14a8-4f6f-8cf4-1ee95ad95b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = gpt2_gen(base_text, max_length=250)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b04b04-51da-4036-9aab-c4cc5d5e0f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/dvgodoy/PyTorchStepByStep/blob/master/Chapter11.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
